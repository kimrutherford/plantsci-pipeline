# -*- org -*-
###+INFOJS_OPT: view:content toc:t ltoc:t mouse:#dddddd
#+OPTIONS:     H:5
#+STYLE: <style type="text/css">  html { font-family: Times, serif; font-size: 16pt; }</style>

University of Cambridge Department of Plant Sciences NGS processing pipeline

* Introduction
The document describes the University of Cambridge Department of Plant
Sciences small read processing pipeline.

It has the following features:
  - it's relatively generic
  - it handles dependencies between jobs
  - results are stored in a database
  - handles sRNA, SAGE, mRNA expression (non-SAGE)
* Requirements
# <<Requirements>>
  - Linux (or UNIX)
  - Perl
  - [[http://www.bioperl.org][BioPerl]] and other Perl modules.  These will be [[Installation][installed automatically]] by
    the build system.
  - PostgreSQL and its development libraries.
    The pipeline was developed using [[http://www.postgresql.org/][PostgreSQL]] 8.3, but any recent version
    should work.
  - An alignment program.  The default is [[http://bioinf.eva.mpg.de/patman/][PatMaN]].
  - SAMtools for generating BAM files, see: http://samtools.sourceforge.net/
* Getting the code
  Currently the easiest way to get the code is via GitHub.  Run this command
  to get a copy of the code:
#+BEGIN_SRC sh
  git clone git://github.com/kimrutherford/plantsci-pipeline.git pipeline
#+END_SRC
  This creates a directory called "pipeline".  The directory can be updated
  later with the command
#+BEGIN_SRC sh
  git pull
#+END_SRC
* Installation
# <<Installation>>
** PostgreSQL libraries
# <<PostgreSQL libraries>>
   The [[http://www.postgresql.org/][PostgreSQL]] server and development libraries need to be installed first.

   On a Debian system the command is:
#+BEGIN_SRC sh
  apt-get install postgresql-8.3 postgresql-server-dev-8.3
#+END_SRC

** Perl code
# <<Perl code>>
Install with the usual Perl installation commands.  This will probably need to
be done as root.  The "make install" step will install any necessary Perl
package dependencies suck as BioPerl.
#+BEGIN_SRC sh
  cd pipeline
  perl Makefile.PL
  make install
#+END_SRC

** Database initialisation
# <<Database initialisation>>
#+BEGIN_SRC sh
  createuser pipe
  createdb -U pipe -h localhost pipeline
  psql -U pipe -h localhost pipeline < etc/schema.sql
#+END_SRC
* Configuration
# <<Configuration>>
** The configuration file
# <<The configuration file>>
   The configuration file is created by the =pipeinit.pl= script and
   contains global configuration.  Other configuration is [[Configuration%20in%20the%20database][stored in the database]].
#+BEGIN_SRC sh
  ./script/pipeinit.pl -f etc/example_deploy_params
#+END_SRC
  This will create a file named =pipeline-example-config.yaml=.
*** =example_deploy_params=
    The settings in this file should be changed to match local conditions.  In
    particular the database parameters will need to be set.
*** Configuration options
# <<Configuration options>>
**** =pipeline_directory=
# <<pipeline_directory>>
     This is the root directory for pipeline operation.  It needs to be
     writable by the user that will run the [[The pipeline server][pipeline server]].
**** =data_sub_directory=
     This a the sub directory of =pipeline_directory= where pipeline results
     will be stored.
**** "Model::SmallRNAModel"
# <<"Model::SmallRNAModel">>
     This section configures the connection from =DBIx::Class= to the
     database.  These options need to be set:
  - =dbname= :: the database name
  - =host= :: the database host
  - =some_user= :: database user name
  - =user_password= :: password for that user
**** =programs=
     Each external program needs a =path= setting - the path to the executable
     for this program.

     A program that is run from the [[SmallRNA_Runable_AlignmentRunable][AlignmentRunable]] will need a
     =process_class= setting to specify the class to use to run the program.
**** =databases=
     This section configures the FASTA sequence databases used in the
     pipeline, mainly by the alignment programs.
***** =root=
      All paths in this section are relative to the =root=, which can be =/=
***** =organisms= / =database_files=
      There may be multiple database files for each organism.  Each file has a
      tag like =genome= or =mrna=.  These tags are referred to as "components"
      in other parts of the code.  In particular the "component" is used in
      the [[proces_conf][=process_conf=]] table to configure which database to use for the
      process.
** Configuration in the database
# <<Configuration in the database>>
   Settings that may need to be configured after the pipeline is running are
   stored in the database.  See the documentation for the following tables:
   [[barcode][=barcode=]], [[barcode_set][=barcode_set=]], [[cvterm][=cvterm=]], [[ecotype][=ecotype=]], [[organisation][=organisation=]], [[organism][=organism=]], [[person][=person=]],
   [[process_conf][=process_conf=]] and [[process_conf_input][=process_conf_input=]]
* Database structure
# <<Database structure>>
** Schema
   The SQL to create the schema is in =etc/schema.sql= of the pipeline source
   code and there are two diagrams of the schema (with different layouts),
   =etc/pipeline_schema_hierachy.ps= and =etc/pipeline_schema_orth.ps=.
** Tables
# <<Database tables>>
*** =barcode=
# <<barcode>>
    Sequence and short name (code) used for multiplexed sequencing runs.

    Columns of this table:
 - =identifier= :: the user friendly identifier of this barcode
 - =code= :: the barcode
*** =barcode_set=
# <<barcode_set>>
    A collection of barcodes that are used together.

    Columns of this table:
 - =name= :: The user friendly name for this =barcode_set=
*** =biosample=
# <<biosample>>
    Each row corresponds to one biological sample.  More than one library can
    be created from one biosample.
    Columns of this table:
*** =biosample_dbxref=
# <<biosample_dbxref>>
    Columns of this table:
*** =biosample_ecotype=
# <<biosample_ecotype>>
    Columns of this table:
*** =biosample_pipedata=
# <<biosample_pipedata>>
    Columns of this table:
*** =biosample_pipeproject=
# <<biosample_pipeproject>>
    Link table that allows many to many relations between projects and biosamples.
*** =cv=
# <<cv>>
    This table holds the controlled vocabulary names.  The table structure is
    copied from the [[http://gmod.org/wiki/Chado][Chado]] [[http://gmod.org/wiki/Chado_CV_Module][CV Module]].
*** =cvterm=
# <<cvterm>>
    This table contains controlled vocabulary terms.  The table structure is
    copied from the [[http://gmod.org/wiki/Chado][Chado]] [[http://gmod.org/wiki/Chado_CV_Module][CV Module]].
*** =cvterm_dbxref=
# <<cvterm_dbxref>>
    A link table connecting references to cvterms.
*** =db=
# <<db>>
    Database table from the [[http://gmod.org/wiki/Chado][Chado]] [[http://gmod.org/wiki/Chado_CV_Module][CV Module]].
*** =dbxref=
# <<dbxref>>
    [[http://gmod.org/wiki/Chado_General_Module#Table:_dbxref][Database cross reference]] table from the [[http://gmod.org/wiki/Chado][Chado]] [[http://gmod.org/wiki/Chado_General_Module][General Module]].
*** =ecotype=
# <<ecotype>>
    An ecotype (also known as a strain or stock) of an [[organism]]

    Columns of this table:
    - =description= :: the common/standard description of the ecotype
*** =library=
# <<library>>
    A library is a biosample that has been prepared for sequencing.  It will
    have adaptors, which may include barcodes.  If there is multiplexing two
    or more libraries will be combined into one sequencing_sample.

    Columns of this table:
 - =biosample= :: the biological sample used to make this library
 - =library_type= :: specifies whether this =library= is an initial
   run or a technical replicate or biological replicate
 - =description= :: an optional description
 - =barcode= :: the optional barcode for this library
 - =adaptor= :: the adaptor used for this library, as stored in the cvterm table
*** =organisation=
# <<organisation>>
    Each person is a member of an =organisation= and sequencing centres are
    organisations in this schema.  A =pipeproject= has an optional =funder=
    that is also an =organisation=.
*** =organism=
# <<organism>>
    Organism table from the [[http://gmod.org/wiki/Chado][Chado]] [[http://gmod.org/wiki/Chado_Organism_Module][Organism Module]].
*** =organism_dbxref=
# <<organism_dbxref>>
    Organism to reference link table from the [[http://gmod.org/wiki/Chado][Chado]] [[http://gmod.org/wiki/Chado_Organism_Module][Organism Module]].
*** =person=
# <<person>>
    This table records information about users of the database.
    Notable columns of this table:
    - =role= :: the role is used by the tracking application to control
      capabilities.
*** =pipedata=
# <<pipedata>>
    The pipeline creates a row in this table for each file that a
    [[Pipeline processes][process]] creates and records the file location (in =file_name=) and its
    size.  This table also tracks the content type and
    format type of the file so that [[Pipeline processes][processes]] can find input files of the
    correct type.

    Columns of this table:
    - =content_type= :: a cvterm specifying the content type.
      eg. "raw_reads", "aligned_reads"
    - =format_type= :: a cvterm recording the format of the file.
      eg. "fasta", "gff3"
    - =file_length= :: the data file length
    - =generating_pipeprocess= :: the id of the [[=pipeprocess=]] that generated
      this =pipedata=
*** =pipedata_property=
# <<pipedata_property>>
    Arbitrary key/value pairs for a =pipedata= entry.

    Columns of this table:
    - =type= :: the key of the pair
    - =value= :: the value as text
    - =pipedata= :: the =pipedata= for the property
*** =pipeprocess=
# <<pipeprocess>>
    This table records the tasks performed by the pipeline.  Each row
    references the =process_conf= that holds the configuration for this
    process.  Pipeline processes are created by the [[The pipeline server][pipeline server]] and each
    process is run by the [[The pipeline worker][pipeline worker]] script (=pipework.pl=).

    Columns of this table:
    - =description= :: a human readable description of the process.
    - =process_conf= :: the =process_conf= that will be used by this
      pipeprocess.
    - =status= :: this is the status field is used by the [[The pipeline server][pipeline server]] and
      will have values from the cvterm table like: "not_started", "started",
      "queued", "finished" or "failed".  See the [[The pipeline server][pipeline server]] section for
      descriptions of each state.
    - =job_identifier= :: the identifier of this process in the job queueing
      system (currently Torque).
    - =time_queued= :: the time when this process moved to the "queued" state,
      otherwise null.
    - =time_started= :: the time when this process moved to the "started" state,
      otherwise null.
    - =time_finished= :: the time when this process moved to the "finished" state,
      otherwise null.
*** =pipeprocess_in_pipedata=
# <<pipeprocess_in_pipedata>>
    This table contains the input files ([[pipedata][=pipedata=]] entries) for the
    pipeprocesses.

    Columns of this table:
*** =pipeprocess_pub=
# <<pipeprocess_pub>>
    Link table from [[pipeprocess][=pipeprocess=]] to [[pub][publications]] to allow articles that
    describe algorithms to be associated with processes within the pipeline.
*** =pipeproject=
# <<pipeproject>>
    A pipeproject is a (potentially loose) collection of biosamples that has a
    name, a description and an owner.  It is linked to the biosample table via
    the biosample_pipeproject table.
*** =process_conf=
# <<process_conf>>
    This table stores configuration for each type of job that can run in the
    pipeline. 

    Columns of this table:
    - =type= :: a cvterm giving the type of this configuration
    - =detail= :: extra details to pass to the [[runables][Runable]]
    - =runable_name= :: The name of the Perl module containing the code to
      perform this process (see [[runables][Runable]])
*** =process_conf_input=
# <<process_conf_input>>
    Columns of this table:
*** =protocol=
# <<protocol>>
    Columns of this table:
*** =pub=
# <<pub>>
*** =pub_dbxref=
# <<pub_dbxref>>
*** =sequencing_sample=
# <<sequencing_sample>>
    Columns of this table:
*** =sequencing_run=
# <<sequencing_run>>
    Columns of this table:
*** =tissue=
# <<tissue>>
    Columns of this table:
* Operation
  The pipeline is run by a [[The pipeline server][server process]] which starts [[The pipeline worker][worker processes]] to do the
  processing.  The server and workers coordinate using a database.  Input
  data, temporary data and output data are stored in the file system, with
  file paths stored in the database.
# <<Operation>>
** Pipeline data
# <<Pipeline data>>
   Files for the pipeline are stored relative to the directory given by the
   [[=pipeline_directory=][=pipeline_directory=]] configuration options, in the sub-directory given by
   the =data_sub_directory= option.  The =file_name= field of the
   [[=pipedata=][=pipedata=]] table will only ever contain paths relative to
   "=<pipeline_directory>/<data_sub_directory>/=".
** The pipeline server
# <<The pipeline server>>
   The script that controls the pipeline is =pipeserv.pl=.  It runs in a loop
   performing these actions:
   - tries to create new processes (ie. entries in the =pipeprocess= table) using
     the [[SmallRNA_ProcessManager][ProcessManager]] class
   - [[Job queuing][queues new jobs]] using [[http://en.wikipedia.org/wiki/Portable_Batch_System][Torque]] or [[http://www.cs.wisc.edu/condor/][Condor]]
   - sleeps, then starts again
** Job queuing
** Pipeline jobs
# <<Pipeline jobs>>
   Each job that the pipeline runs will have an entry in the
   [[=pipeprocess=][pipeprocess]] table.  These entries are initially created by the pipeline
   server (=pipeserv.pl=) and are updated by the pipeline worker
   (=pipework.pl=).
*** Process states
# <<Process states>>
    Entries in the =pipeprocess= table can have the following states:
    - not_started :: Process has not been queued yet - there is an entry in
      the database, but no Torque/Condor job has been created (set by
      =pipeserv.pl=)
    - queued :: A job is queued to run this process - a Torque/Condor job has
      been created (set by =pipeserv.pl=) and is waiting to run
    - started :: Processing has started - Torque/Condor has started running
      this process (set by =pipework.pl=)
    - finished :: Processing is done - the process finished and succeeded (set
      by =pipework.pl=)
    - failed :: Processing failed - the process finished and failed (set by
      =pipework.pl=)
** The pipeline worker
# <<The pipeline worker>>
   Each job is run by =pipework.pl=.  It receives the ID of a [[pipeprocess][=pipeprocess=]] and
   the path to the configuration file as environment variables.
* Implementation
# <<Implementation>>

** Runables
# <<runables>>
   Each pipe_process is executed by a Perl module that is a sub-class of
   =SmallRNA::Runable::SmallRNARunable=.  The [[The pipeline worker][pipeline worker script]] calls
   the =run()= method on the =SmallRNARunable= which does all processing for
   the job.  The =run()= method of the runable should do its processing, write
   its output files to the [[pipeline_directory][=pipeline_directory=]] then call 
   =SmallRNARunable->store_pipedata()= to write the information to the database.

** SmallRNA::Runable::AlignmentRunable
# <<smallrna_runable_alignmentrunable>>
   The Perl module to run to do the alignment is configured using the
   =process_class= setting in the config file
** SmallRNA::ProcessManager
# <<smallrna_processManager>>
   - code for creating [[pipeprocess][=pipeprocess=]] entries
   - looks at dependencies

